<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Permacomputing | initialed85's misc tech stuff</title><meta name=keywords content><meta name=description content="I stumbled across an article on lobste.rs that spoke about permacomputing which interested me initially because I&rsquo;d never heard of it.
As the website suggests the principles are a good place to start.
In particular the lifespan maximization approach resonated with me, here&rsquo;s an excerpt:
Lifespan maximization is the extension of hardware lifespan by the users. It may be supported by planned longevity from the manufacturer&rsquo;s side, but it rarely is."><meta name=author content="Edward Beech"><link rel=canonical href=https://initialed85.cc/posts/permacomputing/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://initialed85.cc/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://initialed85.cc/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://initialed85.cc/favicon-32x32.png><link rel=apple-touch-icon href=https://initialed85.cc/apple-touch-icon.png><link rel=mask-icon href=https://initialed85.cc/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Permacomputing"><meta property="og:description" content="I stumbled across an article on lobste.rs that spoke about permacomputing which interested me initially because I&rsquo;d never heard of it.
As the website suggests the principles are a good place to start.
In particular the lifespan maximization approach resonated with me, here&rsquo;s an excerpt:
Lifespan maximization is the extension of hardware lifespan by the users. It may be supported by planned longevity from the manufacturer&rsquo;s side, but it rarely is."><meta property="og:type" content="article"><meta property="og:url" content="https://initialed85.cc/posts/permacomputing/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-05T21:11:33+08:00"><meta property="article:modified_time" content="2022-10-05T21:11:33+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Permacomputing"><meta name=twitter:description content="I stumbled across an article on lobste.rs that spoke about permacomputing which interested me initially because I&rsquo;d never heard of it.
As the website suggests the principles are a good place to start.
In particular the lifespan maximization approach resonated with me, here&rsquo;s an excerpt:
Lifespan maximization is the extension of hardware lifespan by the users. It may be supported by planned longevity from the manufacturer&rsquo;s side, but it rarely is."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://initialed85.cc/posts/"},{"@type":"ListItem","position":2,"name":"Permacomputing","item":"https://initialed85.cc/posts/permacomputing/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Permacomputing","name":"Permacomputing","description":"I stumbled across an article on lobste.rs that spoke about permacomputing which interested me initially because I\u0026rsquo;d never heard of it.\nAs the website suggests the principles are a good place to start.\nIn particular the lifespan maximization approach resonated with me, here\u0026rsquo;s an excerpt:\nLifespan maximization is the extension of hardware lifespan by the users. It may be supported by planned longevity from the manufacturer\u0026rsquo;s side, but it rarely is.","keywords":[],"articleBody":"I stumbled across an article on lobste.rs that spoke about permacomputing which interested me initially because I’d never heard of it.\nAs the website suggests the principles are a good place to start.\nIn particular the lifespan maximization approach resonated with me, here’s an excerpt:\nLifespan maximization is the extension of hardware lifespan by the users. It may be supported by planned longevity from the manufacturer’s side, but it rarely is.\nFabrication of microchips requires large amounts of energy, highly refined machinery and poisonous substances. Because of this sacrifice, the resulting microchips should be treasured like gems or rare exotic spices. Their active lifespans should be maximized, and they should never be reduced to their raw materials until they are thoroughly unusable.\nBroken devices should be repaired. If the community needs a kind of device that does not exist, it should preferrably be built from existing components that have fallen out of use. Chips should be designed open and flexible, so that they can be reappropriated even for purposes they were never intended for.\nIt got me thinking about the loose long-term plan to migrate my various bits of home software over to a self-hosted Raspberry Pi Kubernetes cluster.\nI walked into my local electronics store maybe 6 months ago with the goal of buying 4 top-of-the-line Raspberry Pis and everything I needed and go get stuck in; however I was informed that there was a Raspberry Pi shortage and the limit was 1 per customer- I went again the other week and was told there simply were none to be bought and they didn’t know when they’d be back in stock.\nTruly we are in unprecedented times.\nRecycling Energized by that particular principle from permacomputing and enforced by the chip shortage, I dug through my shed for all the old Raspberry Pis I’ve accumulated over the years and took stock:\n1 x Raspberry Pi 3 Model B Plus Rev 1.3 3 x Raspberry Pi 3 Model B Rev 1.2 1 x Raspberry Pi 2 Model B v1.1 1 x Raspberry Pi 2011.12 I figured I could probably just ignore the Pi 2 and Pi leaving me with a nice round 4 x Pi 3s to work with; here are the specs of note:\nARM64 4-core Cortex-A53 1200 - 1400 MHz peak pending revision 1 GB RAM In further keeping with the theme, I had a mixture of old external HDDs and SSDs lying around that I figured I could try to turn into some distributed storage.\nBased on frankly not much science and mostly a balance between things I already knew and things I wanted to learn to benefit my professional life, here’s the stack I chose:\nUbuntu Server 22.04 k3s Ceph via Rook Learnings I learned a bunch of things about my existing environment and this hardware and software stack:\n/boot/network-config.txt didn’t seem to seed the netplan stuff as desired Practically this meant I couldn’t do headless installations and needed a screen and keyboard to configure netplan once booted Hacking up /etc/hosts is a pain and I should have properly set up my local DNS server I needed to disable / uninstall a bunch of services to maximise the amount of resources available to Kubernetes ModemManager.service unattended-upgrades.service snapd.service multipathd.service networkd-dispatcher.service bluetooth.service snapd.service snapd.seeded.service snap.lxd.activate.service I needed cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 at /boot/firmware/cmdline.txt I needed to install linux-modules-extra-raspi to enable VXLAN support Because I type quickly with extremely poor accuracy, uninstalling command-not-found python3-commandnotfound saved a bunch of time that I would otherwise have spent waiting for Python to spin up and tell me “hey, command not found pal” This is really only an issue on slow devices- a serious machine gives you a response nice and quickly All my SD cards were too slow for the amount of disk IO that the k3s server wanted to do Moving /var/log to tmpfs didn’t help that much, I think the issue was the SQLite database Moving /var/lib/rancher to an external USB drive helped but not enough Despite needing the RAM, I should have mounted /var/log to tmpfs I lost 2 of the 4 Raspberry Pis to exhausted SD cards in less than a week (both the same sort of SD card though interestingly) Which leads in to my final and most significant learning:\nIt can’t be done Well that’s not really true, here’s a truer statement:\nIt can’t be done if you want to also run a Rook Ceph cluster I basically couldn’t get any of the pods to be happy because everything was timing out all the time; load average was through the roof despite CPU usage not being really that high (suggesting storage was the bottleneck), one of the various Ceph concepts kept running out of memory regardless how I configured it.\nPerhaps “it can’t be done” is a bit too defeatist but a common theme I’ve found over the years is that the universe or the god of computers of whoever is in charge finds initially subtle and then less subtle ways to let me know I’ve taken a wrong turn.\nI had one final attempt to give the Raspberry Pis their best chance by moving the k3s server onto an x86 machine and just having the Raspberry Pis run as agents- still no dice.\nTime for plan B It wouldn’t be a home project without completely falling apart and causing me to lose hours and hours on aspects I hadn’t counted on exploring; how can I recover this effort and continue to learn about Kubernetes while honouring the Permacomputing approach?\nWith old laptops of course! I tracked down the following:\nMy old Asus G55VW which had been sitting on a bench at work for years My old MacBook Pro Late 2013 which I had factory reset and left on a shelf My wife’s old Asus K43e which was stuffed in a drawer in the TV cabinet I had hoped to include my old Alienware M11x R3 but I think one of my coworkers has turned it into a crypto wallet My plan was to couple these laptops with the HP ProLiant MicroServer Gen8 that a coworker gave to me after he migrated our work servers onto some new big flashy servers along with the 4 x Raspberry Pi 3s and build a kind of Beowulf cluster.\nMigration approach The old HP server was already running some piecemeal services as Docker containers or natively:\nDocker Nginx to provide my home landing page and some proxying around my services Eclipse Mosquitto as my MQTT broker Home Assistant for everything (this is an incredibly good and flexible free piece of software) Pi-Hole as a local DNS server Samba to provide a Windows file share for my scanner to upload to (it’s that or give it an email account) Cameranator which is my home-grown CCTV system (Motion, ffmpeg, Hasura and Postgres all glued together with some Go of course) Native A subset of my home-grown mqtt_things that plug into Home Assistant via Mosquitto Ideally I’d like to be able to run all of those services on Kubernetes, but I also want the migration to be as seamless as possible for my users (my wife, who provides me with helpful feedback about our home automation journey such as “I preferred it when I could just turn the light on using the switch on the wall”).\nWith that in mind, my rough plan looked like this:\nGet k3s running on all 8 nodes Use Rook Ceph to turn my old external hard drives into distributed storage Migrate all my services over to Kubernetes without losing any data (except for the rolling 4TB of CCTV footage, I don’t care about that, I figure with few exceptions historic CCTV footage really only needs to be kept long enough to be able to refer to after an incident) There were a few constraints and challenges to manage along the way:\nIt will take me a while to migrate everything, so Kubernetes and my legacy stack must coexist This means I can’t run a Kubernetes load balancer on the HP server until I’m ready Cameranator needs (or least benefits from) access to GPUs (which I have access to with the HP server and some of the laptops) This means I’ll need to manage drivers at the host level and figure out how to expose the GPU at the Kubernetes level One of the mqtt_things instances runs my sprinklers and needs to be plugged into an Arduino via USB serial port This means I’ll need to expose that serial port to Kubernetes and also control which node this thing runs on A recent change to my home ISP has seen port 80 and port 443 blocked on my external IP I’ve used this as an excuse to play with Cloudflare I don’t want to have to refactor any of the software I’ve written This is a pretty artificial limitation, but I struggle to find spare time as it is and I want to dedicate all of that spare time to learning Kubernetes in the short term, so I don’t want to be distracted by refactoring my software to better fit Kubernetes End state diagrams and photos for the impatient Here’s a bit of a logical overview of how everything is shipped:\nHere’s most of the computer cluster sitting next to my TV:\nHere’s 1 of the 2 remaining Raspberry Pis on top of my fridge along with my WAN link:\nAnd here’s the other Raspberry Pi in a box outside (attached to the Arduino for the sprinklers:\nYes, it’s all very slapped together and very dusty but hey, this is a personal project- if I do it properly it’s too much like work ;)\nThe snippets of relevance I’m not gonna go into a great level of detail because it would be pages and pages and honestly I didn’t write enough of it down, so I’ll include just the parts that took me a bit of time and therefore may be of use to somebody.\nSingle-server multi-agent k3s cluster with no load balancer on the server I pre-downloaded the k3s install script referred to here so I could reuse it as I had a feeling I’d be starting from scratch a lot:\ncurl -sfL https://get.k3s.io \u003e k3s-install.sh \u0026\u0026 chmod +x k3s-install.sh The following spins up a k3s node as a server with the kubeconfig file ready to be accessed by any user:\n./k3s-install.sh --write-kubeconfig-mode 644 --node-label svccontroller.k3s.cattle.io/enablelb=false You can dump out the token needed to join k3s agents to the k3s server with the following:\nsudo cat /var/lib/rancher/k3s/server/node-token And then (assuming you’ve downloaded the install script) you can install the k3s agent and join it to your k3s server as follows:\nexport K3S_TOKEN=\"(some big long token)\" export K3S_URL=\"https://(your-server-hostname):6443\" ./k3s-install.sh --node-label svccontroller.k3s.cattle.io/enablelb=true Unrestricted use of a GPU in Kubernetes I lost a lot of time to this one; in theory if you have the drivers installed on the host, you should just be able to deploy the NVIDIA device plugin and away you go- this was not my experience, I’m not sure if this was because of my old GPUs or because of k3s, but I’ll cover what did work for me.\nAs context, I wanted to use my GPUs for transcoding tasks and NVIDIA apply an arbitrary limit on concurrent transcoding sessions for some reason; fortunately some clever individual maintains a tool to patch the drivers that removes that concurrency limit.\nHere are the GPUs I’ve got and the NVIDIA drivers I can run on them:\nQuadro P400 in the HP ProLiant MicroServer Gen8 Supported by 515.76 but probably the more recently released ones too NVIDIA GeForce GTX 660M in the Asus G55VW No support beyond 470.141.03 NVIDIA GeForce GT 650M in the MacBook Pro Late 2013 No support beyond 470.141.03 So, first I installed those drivers, which was basically a case of the following:\nsudo bash ./NVIDIA-Linux-x86_64*.sh I opted for no DKMS, no 32-bit support and basically no frills- I don’t have strong reasons for this, but it has been the most reliable for me on the HP server from back when I was running this workload in Docker (worst case you bump your kernel version and have to reinstall the drivers, after restarting on the new kernel).\nAnyway, I then cloned keylase/nvidia-patch and ran the following:\nsudo bash ./patch.sh In theory now at least at the host level, I have full unrestricted access to my GPUs; to get things working in Kubernetes I followed the prerequisites for the Nvidia device plugin, I skipped making any changes to containerd because k3s seems to do that for you and then I attempted to deploy the plugin without success- it kept insisting it couldn’t find any GPUs.\nFortunately I found a comment on a k3s issue that worked out of the box; I skipped past the setup steps (as that was all sorted) and put together a modified version of hansaya ’s YAML\nYou can find my modified version as a GitHub Gist; the summary of the modifications are:\nRemove the benchmark Add a label requirement (so that discovery won’t happen on nodes unless you label them as gpu=yes) This somewhat defeats the purpose of discovery, but I found the discovery pods just kept crash looping on my non-GPU nodes (both x86 and ARM) So if your host has a happy GPU, you can run the following to make it available to Kubernetes:\nkubectl apply -f https://gist.githubusercontent.com/initialed85/cd8b268b7cad2ca8f991b4087cbbf57a/raw/756335bd68e793e34e56f56dbaff636b8a5cf4e9/nvidia-device-plugin.yml There was another significant gotcha that took me a while to figure out in that somehow my GPUs seemed to be declared as less than 1 GPU per GPU from a resource limits perspective- I had 3 GPUs and 3 pods that needed to do GPU work, if I had each pod ask for 1 GPU, I wouldn’t get more than 1 or 2 happy pods.\nI worked around this with a combination of (unconfirmed how much is required):\nDon’t use resource limits Lean on the same gpu=yes label to lock pods to my GPU nodes Specify runtimeClassName: nvidia at the pod level Specify the environments NVIDIA_VISIBLE_DEVICES=all and NVIDIA_DRIVER_CAPABILITIES=all at the container level With this I was comfortably able to run 6 pods doing GPU work (3 x Motion, 3 x ffmpeg) across 3 different nodes concurrently; additionally, each of those pods is actually made up of 2 containers that use GPU, a primary container that generates video (Motion or events, ffmpeg for segments) and a secondary container that processes video into a low-res variant, so at any given time there are potentially 12 concurrent pieces of GPU work being done aross my cluster (though the reality is Motion events are sparse, so it’s more like 6 - 9 concurrent pieces of GPU work).\nThought: Do I even need the NVIDIA device plugin if I’m handling node selection myself? Who knows, it’s working, I’m worried I’ll break it.\nDeploying a Rook Ceph storage cluster for use by Kubernetes I had initially wanted to try and deploy Ceph natively using cephadm but boy is it complex; a bit of Googling led me to Rook Ceph and I have not looked back, it’s just so good.\nThe key element that took me the longest to work out is that your drives need to be truly empty before Ceph will consume them; you can achieve this as follows:\n# WARNING: be super, super sure you've got the right device path- double-check your fdisk -l and dmesg sudo sgdisk -Z /dev/sdX In some cases you may have to run it twice, or at least I had to run it twice to make it say happy sounding disk things back at me.\nAnyway, assuming you’ve now prepared your drives, the key takeaways to get a Rook Ceph cluster going are something like:\nNeed to install lvm2 on the host first Need at least 3 nodes Then I just followed the quickstart guide and made heavy use of their example repo with minimal tweaks to suit me; as a brief summary that should not just paste into your console without your own investigation:\ngit clone --single-branch --branch v1.10.3 https://github.com/rook/rook.git cd rook/deploy/examples kubectl create -f crds.yaml -f common.yaml -f operator.yaml # keep checking this until rook-operator is in running state kubectl -n rook-ceph get pod # now deploy the cluster kubectl create -f cluster.yaml It takes quite a while for everything to get to a steady state; eventually you should have a stack of pods, if you’ve OSDs (not just the prepare OSD pods) that are happy then you’re good:\n$ kubectl -n rook-ceph get -o wide pods | grep osd | sort rook-ceph-osd-0-59bf5d8c6c-qr9kn 1/1 Running 7 (23h ago) 6d13h 10.42.0.66 romulus rook-ceph-osd-1-54dd746978-hpvjm 1/1 Running 6 (23h ago) 6d13h 10.42.2.225 dionysus rook-ceph-osd-2-699bb5d4bf-296g2 1/1 Running 6 (23h ago) 4d11h 10.42.1.10 chronos rook-ceph-osd-4-6b696fc987-ns22p 1/1 Running 2 (2m28s ago) 27h 10.42.3.143 hera rook-ceph-osd-prepare-chronos-hwj92 0/1 Completed 0 3m22s 10.42.1.35 chronos rook-ceph-osd-prepare-dionysus-ztscc 0/1 Completed 0 3m10s 10.42.2.241 dionysus rook-ceph-osd-prepare-hera-zlvs5 0/1 Completed 0 3m13s 10.42.3.152 hera rook-ceph-osd-prepare-pi-node-1-fq6fr 0/1 Completed 0 3m16s 10.42.5.72 pi-node-1 rook-ceph-osd-prepare-pi-node-2-gm89w 0/1 Completed 0 3m19s 10.42.4.81 pi-node-2 rook-ceph-osd-prepare-romulus-q4bqq 0/1 Completed 0 3m26s 10.42.0.102 romulus I then followed the guide for deploying a shared filesystem and associated storage class which boiled down to:\ncd rook/deploy/examples kubectl create -f filesystem.yaml # wait for the MDS pods kubectl -n rook-ceph get pod -l app=rook-ceph-mds # deploy the storage class kubectl create -f csi/cephfs/storageclass.yaml At this point you’re in a position to make a persistent volume claim against the storage class; something like this:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: cameranator-pvc namespace: cameranator spec: accessModes: - ReadWriteMany resources: requests: storage: 1T storageClassName: rook-cephfs volumeMode: Filesystem And then to make use of that in a pod you’ll want something like this:\napiVersion: apps/v1 kind: Deployment metadata: namespace: cameranator name: postgres-deployment labels: app: postgres spec: replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: volumes: - name: shared-volume persistentVolumeClaim: claimName: cameranator-pvc readOnly: false containers: - name: postgres image: postgres:12 volumeMounts: - name: shared-volume subPath: postgres-volume/var/lib/postgresql/data mountPath: /var/lib/postgresql/data env: - name: POSTGRES_PASSWORD valueFrom: configMapKeyRef: name: postgres-config-map key: POSTGRES_PASSWORD ports: - containerPort: 5432 That’s basically it; check out the Kubernetes stuff for Cameranator to see it all.\nUsing a stateful set for DRY while allocating different configs per replica I’ve got 3 CCTV cameras, 3 GPU nodes and 4-ish concurrent pieces of GPU work per CCTV camera that I need doing- I definitely want to be able to balance that work across my GPU nodes; as far as I could tell, within the limitations of my software architecture I had 3 options:\nManually describe 3 different deployments, one for each CCTV camera This is pretty much the opposite of DRY and invites a fair bit user error as things drift Have something like Helm do that for me Then at least I’ll only have one gross templated deployment full of random symbols stupid intendation (essentially breaking the human facing interface in order to retain the human-facing interface for the non-human side of something that consume YAML) Use a stateful set for maximum DRY However I’ll need to find a way to allocate a different configmap to a stateful set replica by ordinal For my money, the 3rd option is the cleanest provided I can make it work- as always happens, some nice person on the internet has a great suggestion for this that I was able to tweak; basically:\nHave a stateful set of the appropriate scale (in my case 3, one replica per CCTV camera) Have a config map for each CCTV camera and name each config map in the same pattern as the stateful set replicas (e.g. segment-statefulset-0) Have an init container that mounts all config maps to a fixed location and copies the one that matches the hostname of the init container for this replica into an empty mount at a fixed location Have a container that does my actual GPU work and points to the copied config at the fixed location Because the init container always runs before the actual container and because both have the same hostname pattern (as they’re two different containers on the same pod, so same hostname) we can use this pattern to know which file to copy and place in the fixed location.\nHere’s what the heck I am talking about.\nIt all works a treat:\n$ kubectl -n cameranator get -o wide statefulset | grep segment segment-statefulset 3/3 3d16h segment-processor,segment-generator initialed85/cameranator-segment-processor:latest,initialed85/cameranator-segment-generator:latest $ kubectl -n cameranator get -o wide pod | grep segment segment-statefulset-2 2/2 Running 0 23h 10.42.2.237 dionysus segment-statefulset-1 2/2 Running 0 23h 10.42.0.96 romulus segment-statefulset-0 2/2 Running 0 23h 10.42.1.25 chronos $ kubectl -n cameranator exec -it segment-statefulset-0 -c segment-generator -- bash -c \"ps aux | grep ffmpeg | grep -v grep\" root 3010 14.2 1.2 21659692 100932 ? Sl 13:43 84:55 ffmpeg -hwaccel cuda -c:v h264_cuvid -rtsp_transport tcp -i rtsp://192.168.137.31:554/Streaming/Channels/101 -c copy -map 0 -f segment -segment_time 300 -segment_format mp4 -segment_atclocktime 1 -strftime 1 -x264-params keyint=100:scenecut=0 -g 100 -muxdelay 0 -muxpreload 0 -reset_timestamps 1 -c:v h264_nvenc /srv/target_dir/segments/Segment_%Y-%m-%dT%H:%M:%S_Driveway.mp4 $ kubectl -n cameranator exec -it segment-statefulset-1 -c segment-generator -- bash -c \"ps aux | grep ffmpeg | grep -v grep\" root 14 9.6 1.3 9451976 212496 ? Sl 00:34 133:13 ffmpeg -hwaccel cuda -c:v h264_cuvid -rtsp_transport tcp -i rtsp://192.168.137.32:554/Streaming/Channels/101 -c copy -map 0 -f segment -segment_time 300 -segment_format mp4 -segment_atclocktime 1 -strftime 1 -x264-params keyint=100:scenecut=0 -g 100 -muxdelay 0 -muxpreload 0 -reset_timestamps 1 -c:v h264_nvenc /srv/target_dir/segments/Segment_%Y-%m-%dT%H:%M:%S_FrontDoor.mp4 $ kubectl -n cameranator exec -it segment-statefulset-2 -c segment-generator -- bash -c \"ps aux | grep ffmpeg | grep -v grep\" root 13 8.6 1.1 21660656 92312 ? Sl 00:33 119:28 ffmpeg -hwaccel cuda -c:v h264_cuvid -rtsp_transport tcp -i rtsp://192.168.137.33:554/Streaming/Channels/101 -c copy -map 0 -f segment -segment_time 300 -segment_format mp4 -segment_atclocktime 1 -strftime 1 -x264-params keyint=100:scenecut=0 -g 100 -muxdelay 0 -muxpreload 0 -reset_timestamps 1 -c:v h264_nvenc /srv/target_dir/segments/Segment_%Y-%m-%dT%H:%M:%S_SideGate.mp4 It’d be nice if there was some sort of simple API for storing configs that had a stack-like interface; know how many configs you’ve got, know how many replicas you need, each replica’s config is the same, they hit the API and pop a config and all is well.\nSome common routing patterns with Traefik The default ingress controller for k3s is Traefik and as an Nginx guy, I dunno how I feel about it; here’s what I can tell:\nDocumentation is vast and complex yet useless and lacks good examples Community is so-so (hard to find good examples) It seems to get the job done Match on a subdomain and subpath In my case cameranator.initialed85.cc/filebrowser needs to go to filebrowser/filebrowser:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: namespace: cameranator name: filebrowser-ingressroute spec: routes: - match: HostRegexp(`{subdomain:cameranator}.{any:.*}`) \u0026\u0026 PathPrefix(`/filebrowser`) kind: Rule services: - name: filebrowser port: 8080 Match on a subdomain and subpath but strip the subpath before proxying In my case cameranator.initialed85.cc/api needs to go to hasura:\n--- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: namespace: cameranator name: hasura-middleware-stripprefix spec: stripPrefix: prefixes: - /api --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: namespace: cameranator name: hasura-ingressroute spec: routes: - match: HostRegexp(`{subdomain:cameranator}.{any:.*}`) \u0026\u0026 PathPrefix(`/api`) kind: Rule middlewares: - name: hasura-middleware-stripprefix services: - name: hasura port: 8080 Accessing an Arduino via USB serial port This one is pretty easy to be honest; you just describe a volume with a hostPath specified and then mount it to a container as you would anything.\nAdditionally, because there’s only one node that has the Arduino plugged in, I added a label of iot-role=sprinklers to that node and used at as a nodeSelector in my deployment.\nSee here for the details.\nExposing a non-Kubernetes service as a Kubernetes service To ease my migration path, I wanted to keep using my non-Kubernetes MQTT broker until I’m ready to cut it over; this is also very easy.\nYou declare a service as you would for anything but you don’t set any selectors; then you manually declare an endpoint and point it at your non-Kubernetes service and boom, you can hit that thing from anyhwere in your cluster using Kubernetes DNS semantics and plus when you’re ready to cut it over, you don’t have to change anything that’s pointing to it as that interface stays the same.\nHere’s the scoop:\n--- apiVersion: v1 kind: Service metadata: namespace: mqtt-things name: mqtt-broker spec: ports: - protocol: TCP port: 1883 targetPort: 1883 --- apiVersion: v1 kind: Endpoints metadata: namespace: mqtt-things name: mqtt-broker subsets: - addresses: - ip: 192.168.137.253 ports: - port: 1883 And the real code is here.\nMulti-arch Docker builds This one enables me to ship my MQTT stuff either to my x86 or my ARM nodes (except for the Arduino-reliant workload of course); it’s pretty straightforward:\n#!/usr/bin/env bash set -e docker buildx create --use --name mqtt_things function cleanup() { docker buildx rm mqtt_things || true } trap cleanup EXIT function build() { _=${1?:first argument must be CMD_NAME} _=${2?:second argument must be Docker image name part} docker buildx build \\ --platform linux/amd64,linux/arm64 \\ --build-arg CMD_NAME=\"${1}\" \\ -f docker/cli/Dockerfile \\ -t \"initialed85/mqtt-things-${2}:latest\" \\ --push \\ . } build \"sensors_cli\" \"sensors-cli\" build \"smart_aircons_cli\" \"smart-aircons-cli\" build \"sprinklers_cli\" \"sprinklers-cli\" Real code here.\nDeploying, pushing to and pulling from a private Docker registry Until this point I had just been using my free tier Docker Hub repositories but eventually I needed to deploy my landing page Nginx container which has some baked in credentials; the logical choice is to run a private registry.\nFirst, I needed to literally ship the private registry- fortunately there’s a great example as part of the Rook Ceph documentation that of course uses the storage cluster; I made a slight variation that includes an external port 5000 presence.\nkubectl create -f https://gist.githubusercontent.com/initialed85/1db5af337fed4ad9784fee725f551fb5/raw/e9ea67134b6a4bb9ae3257edb9eb1503937ff100/kube-registry.yaml Then you’ll need to modify your build machine’s /etc/hosts to look a bit like this (replacing the IP with one of your node IPs of course):\n192.168.137.253 kube-registry And modify your build machine’s Docker daemon.json or MacOS preference menu equivalent like this:\n{ \"insecure-registries\": [ \"kube-registry:5000\" ] } Ensure on each of your nodes you have a file at /etc/rancher/k3s/registries.yaml that reads as follows:\nmirrors: \"kube-registry:5000\": endpoint: - \"http://kube-registry:5000\" And you’ll also need to edit /ect/hosts on each node to look a bit like this (replacing the IP with that node’s IP of course):\n192.168.137.253\tkube-registry Finally you’re set up to have a build script that looks like this:\n#!/usr/bin/env bash set -e -x pushd \"$(pwd)\" docker build \\ -t kube-registry:5000/nginx-router:latest \\ -f ./Dockerfile \\ . docker push kube-registry:5000/nginx-router:latest And a deployment that looks like this:\napiVersion: apps/v1 kind: Deployment metadata: namespace: nginx-router name: nginx-deployment labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: kube-registry:5000/nginx-router:latest ports: - containerPort: 80 Closing thoughts Honestly, I’m pretty pleased- functionally, my videos are better quality now (having the single HP server do all the recording and transcoding alone seemed to result in weird video artefacts), they load faster now and I think the distributed storage is faster (mixture of SSDs and HDDs) than my dying all-HDD ZFS pool was (fear not, it will live again soon as even more storage given to the storage cluster).\nI’ll migrate the other stuff over in the coming weeks, if I come across anything really interesting I’ll write another article.\nProbable road map stuff:\nI’d like to try to deploy a master-master Postgres cluster I’ll move the Cameranator DB over to it I’ll probably also move Home Assistant over to it I guess I should have some IaC I’ve definitely enjoyed learning about Kubernetes, I think as complex as it is, it’s probably the future.\nStay tuned for 3 months from now when I’m migrating everything to Nomad ho ho ho.\n","wordCount":"4613","inLanguage":"en","datePublished":"2022-10-05T21:11:33+08:00","dateModified":"2022-10-05T21:11:33+08:00","author":{"@type":"Person","name":"Edward Beech"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://initialed85.cc/posts/permacomputing/"},"publisher":{"@type":"Organization","name":"initialed85's misc tech stuff","logo":{"@type":"ImageObject","url":"https://initialed85.cc/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://initialed85.cc/ accesskey=h title="initialed85's misc tech stuff (Alt + H)">initialed85's misc tech stuff</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Permacomputing</h1><div class=post-meta><span title='2022-10-05 21:11:33 +0800 +0800'>October 5, 2022</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;Edward Beech</div></header><div class=post-content><p>I stumbled across <a href=https://j3s.sh/thought/drones-run-linux-free-software-isnt-enough.html>an article on lobste.rs</a> that spoke
about <a href=https://permacomputing.net/>permacomputing</a> which interested me initially because I&rsquo;d never heard of it.</p><p>As the website suggests <a href=https://permacomputing.net/Principles/>the principles</a> are a good place to start.</p><p>In particular the <a href=https://permacomputing.net/lifespan_maximization/>lifespan maximization</a> approach resonated with me, here&rsquo;s an excerpt:</p><blockquote><p>Lifespan maximization is the extension of hardware lifespan by the users. It may be supported by planned longevity from the manufacturer&rsquo;s
side, but it rarely is.</p><p>Fabrication of microchips requires large amounts of energy, highly refined machinery and poisonous substances. Because of this sacrifice,
the resulting microchips should be treasured like gems or rare exotic spices. Their active lifespans should be maximized, and they should
never be reduced to their raw materials until they are thoroughly unusable.</p><p>Broken devices should be repaired. If the community needs a kind of device that does not exist, it should preferrably be built from
existing components that have fallen out of use. Chips should be designed open and flexible, so that they can be reappropriated even for
purposes they were never intended for.</p></blockquote><p>It got me thinking about the loose long-term plan to migrate my various bits of home software over to a self-hosted Raspberry Pi Kubernetes
cluster.</p><p>I walked into my local electronics store maybe 6 months ago with the goal of buying 4 top-of-the-line Raspberry Pis and everything I needed
and go get stuck in; however I was informed that there was
a <a href=https://www.jeffgeerling.com/blog/2022/you-cant-buy-raspberry-pi-right-now>Raspberry Pi shortage</a> and the limit was 1 per customer- I
went again the other week and was told there simply were none to be bought and they didn&rsquo;t know when they&rsquo;d be back in stock.</p><p>Truly we are
in <a href=https://www.chicagotribune.com/coronavirus/ct-ent-coronavirus-language-cliches-20200506-kkxibytnprgmfiynjjepevypwu-story.html>unprecedented times.</a></p><h2 id=recycling>Recycling<a hidden class=anchor aria-hidden=true href=#recycling>#</a></h2><p>Energized by that particular principle from permacomputing and enforced by the chip shortage, I dug through my shed for all the old
Raspberry Pis I&rsquo;ve accumulated over the years and took stock:</p><ul><li>1 x <a href=https://www.raspberrypi.com/products/raspberry-pi-3-model-b/>Raspberry Pi 3 Model B</a> Plus Rev 1.3</li><li>3 x <a href=https://www.raspberrypi.com/products/raspberry-pi-3-model-b/>Raspberry Pi 3 Model B</a> Rev 1.2</li><li>1 x <a href=https://www.raspberrypi.com/products/raspberry-pi-2-model-b/>Raspberry Pi 2 Model B</a> v1.1</li><li>1 x <a href=https://www.raspberrypi.com/products/raspberry-pi-1-model-b-plus/>Raspberry Pi</a> 2011.12</li></ul><p>I figured I could probably just ignore the Pi 2 and Pi leaving me with a nice round 4 x Pi 3s to work with; here are the specs of note:</p><ul><li>ARM64</li><li>4-core Cortex-A53</li><li>1200 - 1400 MHz peak pending revision</li><li>1 GB RAM</li></ul><p>In further keeping with the theme, I had a mixture of old external HDDs and SSDs lying around that I figured I could try to turn into some
distributed storage.</p><p>Based on frankly not much science and mostly a balance between things I already knew and things I wanted to learn to benefit my professional
life, here&rsquo;s the stack I chose:</p><ul><li><a href=https://ubuntu.com/download/raspberry-pi>Ubuntu Server 22.04</a></li><li><a href=https://docs.k3s.io/quick-start>k3s</a></li><li><a href=https://ceph.io/en/>Ceph</a> via <a href=https://rook.io/docs/rook/v1.10/Getting-Started/quickstart/#create-a-ceph-cluster>Rook</a></li></ul><h2 id=learnings>Learnings<a hidden class=anchor aria-hidden=true href=#learnings>#</a></h2><p>I learned a bunch of things about my existing environment and this hardware and software stack:</p><ul><li><code>/boot/network-config.txt</code> didn&rsquo;t seem to seed the <code>netplan</code> stuff as desired<ul><li>Practically this meant I couldn&rsquo;t do headless installations and needed a screen and keyboard to configure <code>netplan</code> once booted</li></ul></li><li>Hacking up <code>/etc/hosts</code> is a pain and I should have properly set up my local DNS server</li><li>I needed to disable / uninstall a bunch of services to maximise the amount of resources available to Kubernetes<ul><li><code>ModemManager.service</code></li><li><code>unattended-upgrades.service</code></li><li><code>snapd.service</code></li><li><code>multipathd.service</code></li><li><code>networkd-dispatcher.service</code></li><li><code>bluetooth.service</code></li><li><code>snapd.service</code></li><li><code>snapd.seeded.service</code></li><li><code>snap.lxd.activate.service</code></li></ul></li><li>I needed <code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1</code> at <code>/boot/firmware/cmdline.txt</code></li><li>I needed to install <code>linux-modules-extra-raspi</code> to enable VXLAN support</li><li>Because I type quickly with extremely poor accuracy, uninstalling <code>command-not-found python3-commandnotfound</code> saved a bunch of time that I
would otherwise have spent waiting for Python to spin up and tell me &ldquo;hey, command not found pal&rdquo;<ul><li>This is really only an issue on slow devices- a serious machine gives you a response nice and quickly</li></ul></li><li>All my SD cards were too slow for the amount of disk IO that the k3s server wanted to do<ul><li>Moving <code>/var/log</code> to <code>tmpfs</code> didn&rsquo;t help that much, I think the issue was the SQLite database</li><li>Moving <code>/var/lib/rancher</code> to an external USB drive helped but not enough</li></ul></li><li>Despite needing the RAM, I should have mounted <code>/var/log</code> to <code>tmpfs</code><ul><li>I lost 2 of the 4 Raspberry Pis to exhausted SD cards in less than a week (both the same sort of SD card though interestingly)</li></ul></li></ul><p>Which leads in to my final and most significant learning:</p><ul><li><strong>It can&rsquo;t be done</strong></li></ul><p>Well that&rsquo;s not really true, here&rsquo;s a truer statement:</p><ul><li><strong>It can&rsquo;t be done</strong> <em>if you want to also run a <a href=https://rook.io/docs/rook/v1.10/Getting-Started/intro/>Rook Ceph</a> cluster</em></li></ul><p>I basically couldn&rsquo;t get any of the pods to be happy because everything was timing out all the time; load average was through the roof
despite CPU usage not being really that high (suggesting storage was the bottleneck), one of the various Ceph concepts kept running out of
memory regardless how I configured it.</p><p>Perhaps &ldquo;it can&rsquo;t be done&rdquo; is a bit too defeatist but a common theme I&rsquo;ve found over the years is that the universe or the god of computers
of whoever is in charge finds initially subtle and then less subtle ways to let me know I&rsquo;ve taken a wrong turn.</p><p>I had one final attempt to give the Raspberry Pis their best chance by moving the k3s server onto an x86 machine and just having the
Raspberry Pis run as agents- still no dice.</p><h2 id=time-for-plan-b>Time for plan B<a hidden class=anchor aria-hidden=true href=#time-for-plan-b>#</a></h2><p>It wouldn&rsquo;t be a home project without completely falling apart and causing me to lose hours and hours on aspects I hadn&rsquo;t counted on
exploring; how can I recover this effort and continue to learn about Kubernetes while honouring the Permacomputing approach?</p><p>With old laptops of course! I tracked down the following:</p><ul><li>My old <a href=https://rog.asus.com/notebook/15-inch/g55vw/>Asus G55VW</a> which had been sitting on a bench at work for years</li><li>My old <a href="https://support.apple.com/kb/sp691?locale=en_AU">MacBook Pro Late 2013</a> which I had factory reset and left on a shelf</li><li>My wife&rsquo;s old <a href=http://rmromero.blogspot.com/2011/03/asus-k43e-specifications.html>Asus K43e</a> which was stuffed in a drawer in the TV
cabinet</li><li>I had hoped to include my old <a href=https://www.anandtech.com/show/4505/alienware-m11x-r3-portable-powerhouse>Alienware M11x R3</a> but I think
one of my coworkers has turned it into a crypto wallet</li></ul><p>My plan was to couple these laptops with
the <a href="https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-c03793258">HP ProLiant MicroServer Gen8</a>
that <a href=https://84ace.com/>a coworker</a> gave to me after he migrated our work servers onto some new big flashy servers along with the 4 x
Raspberry Pi 3s and build a kind of <a href=https://en.wikipedia.org/wiki/Beowulf_cluster>Beowulf cluster</a>.</p><h2 id=migration-approach>Migration approach<a hidden class=anchor aria-hidden=true href=#migration-approach>#</a></h2><p>The old HP server was already running some piecemeal services as Docker containers or natively:</p><ul><li>Docker<ul><li><a href=https://www.nginx.com/>Nginx</a> to provide my home landing page and some proxying around my services</li><li><a href=https://mosquitto.org/>Eclipse Mosquitto</a> as my MQTT broker</li><li><a href=https://www.home-assistant.io/>Home Assistant</a> for everything (this is an incredibly good and flexible free piece of software)</li><li><a href=https://pi-hole.net/>Pi-Hole</a> as a local DNS server</li><li><a href=https://www.samba.org/>Samba</a> to provide a Windows file share for my scanner to upload to (it&rsquo;s that or give it an email account)</li><li><a href=https://github.com/initialed85/cameranator>Cameranator</a> which is my home-grown CCTV
system (<a href=https://github.com/Motion-Project/motion>Motion</a>, <a href=https://www.ffmpeg.org/ffmpeg.html>ffmpeg</a>, <a href=https://hasura.io/>Hasura</a>
and <a href=https://www.postgresql.org/>Postgres</a> all glued together with some <a href=https://go.dev/>Go</a> of course)</li></ul></li><li>Native<ul><li>A subset of my home-grown <a href=https://github.com/initialed85/mqtt_things>mqtt_things</a> that plug into Home Assistant via Mosquitto</li></ul></li></ul><p>Ideally I&rsquo;d like to be able to run all of those services on Kubernetes, but I also want the migration to be as seamless as possible for my
users (my wife, who provides me with helpful feedback about our home automation journey such as &ldquo;I preferred it when I could just turn the
light on using the switch on the wall&rdquo;).</p><p>With that in mind, my rough plan looked like this:</p><ul><li>Get k3s running on all 8 nodes</li><li>Use <a href=https://rook.io/docs/rook/v1.10/Getting-Started/intro/>Rook Ceph</a> to turn my old external hard drives into distributed storage</li><li>Migrate all my services over to Kubernetes without losing any data (except for the rolling 4TB of CCTV footage, I don&rsquo;t care about that, I
figure with few exceptions historic CCTV footage really only needs to be kept long enough to be able to refer to after an incident)</li></ul><p>There were a few constraints and challenges to manage along the way:</p><ul><li>It will take me a while to migrate everything, so Kubernetes and my legacy stack must coexist<ul><li>This means I can&rsquo;t run a Kubernetes load balancer on the HP server until I&rsquo;m ready</li></ul></li><li>Cameranator needs (or least benefits from) access to GPUs (which I have access to with the HP server and some of the laptops)<ul><li>This means I&rsquo;ll need to manage drivers at the host level and figure out how to expose the GPU at the Kubernetes level</li></ul></li><li>One of the <code>mqtt_things</code> instances runs my sprinklers and needs to be plugged into an Arduino via USB serial port<ul><li>This means I&rsquo;ll need to expose that serial port to Kubernetes and also control which node this thing runs on</li></ul></li><li>A recent change to my home ISP has seen port 80 and port 443 blocked on my external IP<ul><li>I&rsquo;ve used this as an excuse to play with <a href=https://www.cloudflare.com/en-au/>Cloudflare</a></li></ul></li><li>I don&rsquo;t want to have to refactor any of the software I&rsquo;ve written<ul><li>This is a pretty artificial limitation, but I struggle to find spare time as it is and I want to dedicate all of that spare time to
learning Kubernetes in the short term, so I don&rsquo;t want to be distracted by refactoring my software to better fit Kubernetes</li></ul></li></ul><h2 id=end-state-diagrams-and-photos-for-the-impatient>End state diagrams and photos for the impatient<a hidden class=anchor aria-hidden=true href=#end-state-diagrams-and-photos-for-the-impatient>#</a></h2><p>Here&rsquo;s a bit of a logical overview of how everything is shipped:</p><p><img loading=lazy src=/posts/permacomputing/image-1.png alt="Image 1"></p><p>Here&rsquo;s most of the computer cluster sitting next to my TV:</p><p><img loading=lazy src=/posts/permacomputing/image-2.png alt="Image 2"></p><p>Here&rsquo;s 1 of the 2 remaining Raspberry Pis on top of my fridge along with my WAN link:</p><p><img loading=lazy src=/posts/permacomputing/image-3.png alt="Image 3"></p><p>And here&rsquo;s the other Raspberry Pi in a box outside (attached to
the <a href=https://github.com/initialed85/mqtt_things/tree/master/res/arduino/setup>Arduino for the sprinklers</a>:</p><p><img loading=lazy src=/posts/permacomputing/image-4.png alt="Image 4"></p><p>Yes, it&rsquo;s all very slapped together and very dusty but hey, this is a personal project- if I do it properly it&rsquo;s too much like work ;)</p><h2 id=the-snippets-of-relevance>The snippets of relevance<a hidden class=anchor aria-hidden=true href=#the-snippets-of-relevance>#</a></h2><p>I&rsquo;m not gonna go into a great level of detail because it would be pages and pages and honestly I didn&rsquo;t write enough of it down, so I&rsquo;ll
include just the parts that took me a bit of time and therefore may be of use to somebody.</p><h3 id=single-server-multi-agent-k3s-cluster-with-no-load-balancer-on-the-server>Single-server multi-agent k3s cluster with no load balancer on the server<a hidden class=anchor aria-hidden=true href=#single-server-multi-agent-k3s-cluster-with-no-load-balancer-on-the-server>#</a></h3><p>I pre-downloaded the <a href=https://docs.k3s.io/quick-start>k3s install script referred to here</a> so I could reuse it as I had a feeling I&rsquo;d be
starting from scratch a lot:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -sfL https://get.k3s.io &gt; k3s-install.sh <span style=color:#f92672>&amp;&amp;</span> chmod +x k3s-install.sh
</span></span></code></pre></div><p>The following spins up a k3s node as a server with
the <a href=https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/>kubeconfig</a> file ready to be accessed by any
user:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./k3s-install.sh --write-kubeconfig-mode <span style=color:#ae81ff>644</span> --node-label svccontroller.k3s.cattle.io/enablelb<span style=color:#f92672>=</span>false
</span></span></code></pre></div><p>You can dump out the token needed to join k3s agents to the k3s server with the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo cat /var/lib/rancher/k3s/server/node-token
</span></span></code></pre></div><p>And then (assuming you&rsquo;ve downloaded the install script) you can install the k3s agent and join it to your k3s server as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>export K3S_TOKEN<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;(some big long token)&#34;</span>
</span></span><span style=display:flex><span>export K3S_URL<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;https://(your-server-hostname):6443&#34;</span>
</span></span><span style=display:flex><span>./k3s-install.sh --node-label svccontroller.k3s.cattle.io/enablelb<span style=color:#f92672>=</span>true
</span></span></code></pre></div><h3 id=unrestricted-use-of-a-gpu-in-kubernetes>Unrestricted use of a GPU in Kubernetes<a hidden class=anchor aria-hidden=true href=#unrestricted-use-of-a-gpu-in-kubernetes>#</a></h3><p>I lost a lot of time to this one; in theory if you have the drivers installed on the host, you should just be able to deploy
the <a href=https://github.com/NVIDIA/k8s-device-plugin>NVIDIA device plugin</a> and away you go- this was not my experience, I&rsquo;m not sure if this
was because of my old GPUs or because of k3s, but I&rsquo;ll cover what did work for me.</p><p>As context, I wanted to use my GPUs for transcoding tasks and
NVIDIA <a href=https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new>apply an arbitrary limit on concurrent transcoding sessions</a>
for some reason; fortunately <a href=https://github.com/keylase>some clever individual</a>
maintains <a href=https://github.com/keylase/nvidia-patch>a tool to patch the drivers</a> that removes that concurrency limit.</p><p>Here are the GPUs I&rsquo;ve got and the <a href=https://github.com/keylase/nvidia-patch#version-table>NVIDIA drivers</a> I can run on them:</p><ul><li><a href=https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/productspage/quadro/quadro-desktop/quadro-pascal-p400-data-sheet-us-nv-704503-r1.pdf>Quadro P400</a>
in the HP ProLiant MicroServer Gen8<ul><li>Supported by <a href=https://international.download.nvidia.com/XFree86/Linux-x86_64/515.76/NVIDIA-Linux-x86_64-515.76.run>515.76</a> but
probably the more recently released ones too</li></ul></li><li><a href=https://www.nvidia.com/en-us/geforce/gaming-laptops/geforce-gtx-660m/specifications/>NVIDIA GeForce GTX 660M</a> in the Asus G55VW<ul><li>No support
beyond <a href=https://international.download.nvidia.com/XFree86/Linux-x86_64/470.141.03/NVIDIA-Linux-x86_64-470.141.03.run>470.141.03</a></li></ul></li><li><a href=https://www.nvidia.com/en-gb/geforce/gaming-laptops/geforce-gt-650m/features/>NVIDIA GeForce GT 650M</a> in the MacBook Pro Late 2013<ul><li>No support
beyond <a href=https://international.download.nvidia.com/XFree86/Linux-x86_64/470.141.03/NVIDIA-Linux-x86_64-470.141.03.run>470.141.03</a></li></ul></li></ul><p>So, first I installed those drivers, which was basically a case of the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo bash ./NVIDIA-Linux-x86_64*.sh
</span></span></code></pre></div><p>I opted for no DKMS, no 32-bit support and basically no frills- I don&rsquo;t have strong reasons for this, but it has been the most reliable for
me on the HP server from back when I was running this workload in Docker (worst case you bump your kernel version and have to reinstall the
drivers, after restarting on the new kernel).</p><p>Anyway, I then cloned <a href=https://github.com/keylase/nvidia-patch>keylase/nvidia-patch</a> and ran the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo bash ./patch.sh
</span></span></code></pre></div><p>In theory now at least at the host level, I have full unrestricted access to my GPUs; to get things working in Kubernetes I
followed <a href=https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes>the prerequisites</a> for the Nvidia device plugin, I skipped
making any changes to <code>containerd</code> because k3s seems to do that for you and then I attempted
to <a href=https://github.com/NVIDIA/k8s-device-plugin#enabling-gpu-support-in-kubernetes>deploy the plugin</a> without success- it kept insisting it
couldn&rsquo;t find any GPUs.</p><p>Fortunately I found <a href=https://github.com/k3s-io/k3s/issues/4391#issuecomment-1194627754>a comment on a k3s issue</a> that worked out of the
box; I skipped past the setup steps (as that was all sorted) and put together a modified version
of <a href=https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.12.2/nvidia-device-plugin.yml>hansaya &rsquo;s YAML</a></p><p>You can find <a href=https://gist.github.com/initialed85/cd8b268b7cad2ca8f991b4087cbbf57a>my modified version as a GitHub Gist</a>; the summary of
the modifications are:</p><ul><li>Remove the benchmark</li><li>Add a label requirement (so that discovery won&rsquo;t happen on nodes unless you label them as <code>gpu=yes</code>)<ul><li>This somewhat defeats the purpose of discovery, but I found the discovery pods just kept crash looping on my non-GPU nodes (both x86
and ARM)</li></ul></li></ul><p>So if your host has a happy GPU, you can run the following to make it available to Kubernetes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://gist.githubusercontent.com/initialed85/cd8b268b7cad2ca8f991b4087cbbf57a/raw/756335bd68e793e34e56f56dbaff636b8a5cf4e9/nvidia-device-plugin.yml
</span></span></code></pre></div><p>There was another significant gotcha that took me a while to figure out in that somehow my GPUs seemed to be declared as less than 1 GPU per
GPU from a resource limits perspective- I had 3 GPUs and 3 pods that needed to do GPU work, if I had each pod ask for 1 GPU, I wouldn&rsquo;t get
more than 1 or 2 happy pods.</p><p>I worked around this with a combination of (unconfirmed how much is required):</p><ul><li>Don&rsquo;t use resource limits</li><li>Lean on the same <code>gpu=yes</code> label to lock pods to my GPU nodes</li><li>Specify <code>runtimeClassName: nvidia</code> at the pod level</li><li>Specify the environments <code>NVIDIA_VISIBLE_DEVICES=all</code> and <code>NVIDIA_DRIVER_CAPABILITIES=all</code> at the container level</li></ul><p>With this I was comfortably able to run 6 pods doing GPU work (3 x Motion, 3 x ffmpeg) across 3 different nodes concurrently; additionally,
each of those pods is actually made up of 2 containers that use GPU, a primary container that generates video (Motion or events, ffmpeg for
segments) and a secondary container that processes video into a low-res variant, so at any given time there are potentially 12 concurrent
pieces of GPU work being done aross my cluster (though the reality is Motion events are sparse, so it&rsquo;s more like 6 - 9 concurrent pieces of
GPU work).</p><p>Thought: Do I even need the NVIDIA device plugin if I&rsquo;m handling node selection myself? Who knows, it&rsquo;s working, I&rsquo;m worried I&rsquo;ll break it.</p><h3 id=deploying-a-rook-ceph-storage-cluster-for-use-by-kubernetes>Deploying a Rook Ceph storage cluster for use by Kubernetes<a hidden class=anchor aria-hidden=true href=#deploying-a-rook-ceph-storage-cluster-for-use-by-kubernetes>#</a></h3><p>I had initially wanted to try and deploy Ceph natively using <a href=https://docs.ceph.com/en/quincy/cephadm/index.html>cephadm</a> but boy is it
complex; a bit of Googling led me to <a href=https://rook.io/docs/rook/v1.10/Getting-Started/intro/>Rook Ceph</a> and I have not looked back, it&rsquo;s
just so good.</p><p>The key element that took me the longest to work out is that your drives need to be truly empty before Ceph will consume them; you can
achieve this as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e># WARNING: be super, super sure you&#39;ve got the right device path- double-check your fdisk -l and dmesg</span>
</span></span><span style=display:flex><span>sudo sgdisk -Z /dev/sdX
</span></span></code></pre></div><p>In some cases you may have to run it twice, or at least I had to run it twice to make it say happy sounding disk things back at me.</p><p>Anyway, assuming you&rsquo;ve now prepared your drives, the key takeaways to get a Rook Ceph cluster going are something like:</p><ul><li>Need to install <code>lvm2</code> <strong>on the host</strong> first</li><li>Need at least 3 nodes</li></ul><p>Then I just followed the <a href=https://rook.io/docs/rook/v1.10/Getting-Started/quickstart/>quickstart guide</a> and made heavy use of their example
repo with minimal tweaks to suit me; as a brief summary that <strong>should not</strong> just paste into your console without your own investigation:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>git clone --single-branch --branch v1.10.3 https://github.com/rook/rook.git
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cd rook/deploy/examples
</span></span><span style=display:flex><span>kubectl create -f crds.yaml -f common.yaml -f operator.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># keep checking this until rook-operator is in running state</span>
</span></span><span style=display:flex><span>kubectl -n rook-ceph get pod
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># now deploy the cluster</span>
</span></span><span style=display:flex><span>kubectl create -f cluster.yaml
</span></span></code></pre></div><p>It takes quite a while for everything to get to a steady state; eventually you should have a stack of pods, if you&rsquo;ve OSDs (not just the
prepare OSD pods) that are happy then you&rsquo;re good:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ kubectl -n rook-ceph get -o wide pods | grep osd | sort
</span></span><span style=display:flex><span>rook-ceph-osd-0-59bf5d8c6c-qr9kn                     1/1     Running     <span style=color:#ae81ff>7</span> <span style=color:#f92672>(</span>23h ago<span style=color:#f92672>)</span>     6d13h   10.42.0.66        romulus     &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-1-54dd746978-hpvjm                     1/1     Running     <span style=color:#ae81ff>6</span> <span style=color:#f92672>(</span>23h ago<span style=color:#f92672>)</span>     6d13h   10.42.2.225       dionysus    &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-2-699bb5d4bf-296g2                     1/1     Running     <span style=color:#ae81ff>6</span> <span style=color:#f92672>(</span>23h ago<span style=color:#f92672>)</span>     4d11h   10.42.1.10        chronos     &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-4-6b696fc987-ns22p                     1/1     Running     <span style=color:#ae81ff>2</span> <span style=color:#f92672>(</span>2m28s ago<span style=color:#f92672>)</span>   27h     10.42.3.143       hera        &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-chronos-hwj92                  0/1     Completed   <span style=color:#ae81ff>0</span>               3m22s   10.42.1.35        chronos     &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-dionysus-ztscc                 0/1     Completed   <span style=color:#ae81ff>0</span>               3m10s   10.42.2.241       dionysus    &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-hera-zlvs5                     0/1     Completed   <span style=color:#ae81ff>0</span>               3m13s   10.42.3.152       hera        &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-pi-node-1-fq6fr                0/1     Completed   <span style=color:#ae81ff>0</span>               3m16s   10.42.5.72        pi-node-1   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-pi-node-2-gm89w                0/1     Completed   <span style=color:#ae81ff>0</span>               3m19s   10.42.4.81        pi-node-2   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>rook-ceph-osd-prepare-romulus-q4bqq                  0/1     Completed   <span style=color:#ae81ff>0</span>               3m26s   10.42.0.102       romulus     &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><p>I then followed the guide for
deploying <a href=https://rook.io/docs/rook/v1.10/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/>a shared filesystem</a> and
associated <a href=https://kubernetes.io/docs/concepts/storage/storage-classes/>storage class</a> which boiled down to:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cd rook/deploy/examples
</span></span><span style=display:flex><span>kubectl create -f filesystem.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># wait for the MDS pods</span>
</span></span><span style=display:flex><span>kubectl -n rook-ceph get pod -l app<span style=color:#f92672>=</span>rook-ceph-mds
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># deploy the storage class</span>
</span></span><span style=display:flex><span>kubectl create -f csi/cephfs/storageclass.yaml
</span></span></code></pre></div><p>At this point you&rsquo;re in a position to make
a <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>persistent volume claim</a> against the storage
class; something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolumeClaim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cameranator-pvc</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>cameranator</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteMany</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>1T</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>rook-cephfs</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumeMode</span>: <span style=color:#ae81ff>Filesystem</span>
</span></span></code></pre></div><p>And then to <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes>make use of that in a pod</a> you&rsquo;ll want
something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>cameranator</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>postgres-deployment</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>postgres</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>postgres</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>postgres</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>shared-volume</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>persistentVolumeClaim</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>claimName</span>: <span style=color:#ae81ff>cameranator-pvc</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>readOnly</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>postgres</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>image</span>: <span style=color:#ae81ff>postgres:12</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>shared-volume</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>subPath</span>: <span style=color:#ae81ff>postgres-volume/var/lib/postgresql/data</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/var/lib/postgresql/data</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>POSTGRES_PASSWORD</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>valueFrom</span>:
</span></span><span style=display:flex><span>                <span style=color:#f92672>configMapKeyRef</span>:
</span></span><span style=display:flex><span>                  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>postgres-config-map</span>
</span></span><span style=display:flex><span>                  <span style=color:#f92672>key</span>: <span style=color:#ae81ff>POSTGRES_PASSWORD</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>5432</span>
</span></span></code></pre></div><p>That&rsquo;s basically it; check out the <a href=https://github.com/initialed85/cameranator/tree/master/kubernetes>Kubernetes stuff for Cameranator</a> to
see it all.</p><h3 id=using-a-stateful-set-for-dry-while-allocating-different-configs-per-replica>Using a stateful set for DRY while allocating different configs per replica<a hidden class=anchor aria-hidden=true href=#using-a-stateful-set-for-dry-while-allocating-different-configs-per-replica>#</a></h3><p>I&rsquo;ve got 3 CCTV cameras, 3 GPU nodes and 4-ish concurrent pieces of GPU work per CCTV camera that I need doing- I definitely want to be able
to balance that work across my GPU nodes; as far as I could tell, within the limitations of my software architecture I had 3 options:</p><ul><li>Manually describe 3 different <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/>deployments</a>, one for each CCTV
camera<ul><li>This is pretty much the opposite of DRY and invites a fair bit user error as things drift</li></ul></li><li>Have something like <a href=https://helm.sh/>Helm</a> do that for me<ul><li>Then at least I&rsquo;ll only have one gross templated deployment full of random symbols stupid intendation (essentially breaking the human
facing interface in order to retain the human-facing interface for the non-human side of something that consume YAML)</li></ul></li><li>Use a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/>stateful set</a> for maximum DRY<ul><li>However I&rsquo;ll need to find a way to allocate a different configmap to a stateful set replica by ordinal</li></ul></li></ul><p>For my money, the 3rd option is the cleanest provided I can make it work- as always happens, some nice person on the internet <a href=https://itnext.io/kubernetes-statefulset-initialization-with-unique-configs-per-pod-7e02c01ada65>has a great
suggestion for this</a> that I was able to
tweak; basically:</p><ul><li>Have a stateful set of the appropriate scale (in my case 3, one replica per CCTV camera)</li><li>Have a <a href=https://kubernetes.io/docs/concepts/configuration/configmap/>config map</a> for each CCTV camera and name each config map in the
same pattern as the stateful set replicas (e.g. <code>segment-statefulset-0</code>)</li><li>Have an <a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>init container</a> that mounts all config maps to a fixed
location and copies the one that matches the hostname of the init container for this replica into an empty mount at a fixed location</li><li>Have a <a href=https://kubernetes.io/docs/concepts/containers/>container</a> that does my actual GPU work and points to the copied config at the
fixed location</li></ul><p>Because the init container always runs before the actual container and because both have the same hostname pattern (as they&rsquo;re two different
containers on the same pod, so same hostname) we can use this pattern to know which file to copy and place in the fixed location.</p><p>Here&rsquo;s <a href=https://github.com/initialed85/cameranator/blob/master/kubernetes/segment.yaml>what the heck I am talking about</a>.</p><p>It all works a treat:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ kubectl -n cameranator get -o wide statefulset | grep segment
</span></span><span style=display:flex><span>segment-statefulset   3/3     3d16h   segment-processor,segment-generator   initialed85/cameranator-segment-processor:latest,initialed85/cameranator-segment-generator:latest
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl -n cameranator get -o wide pod | grep segment
</span></span><span style=display:flex><span>segment-statefulset-2                     2/2     Running   <span style=color:#ae81ff>0</span>              23h     10.42.2.237   dionysus   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>segment-statefulset-1                     2/2     Running   <span style=color:#ae81ff>0</span>              23h     10.42.0.96    romulus    &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>segment-statefulset-0                     2/2     Running   <span style=color:#ae81ff>0</span>              23h     10.42.1.25    chronos    &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl -n cameranator exec -it segment-statefulset-0 -c segment-generator -- bash -c <span style=color:#e6db74>&#34;ps aux | grep ffmpeg | grep -v grep&#34;</span>
</span></span><span style=display:flex><span>root        <span style=color:#ae81ff>3010</span> 14.2  1.2 <span style=color:#ae81ff>21659692</span> <span style=color:#ae81ff>100932</span> ?     Sl   13:43  84:55 ffmpeg -hwaccel cuda -c:v h264_cuvid -rtsp_transport tcp -i rtsp://192.168.137.31:554/Streaming/Channels/101 -c copy -map <span style=color:#ae81ff>0</span> -f segment -segment_time <span style=color:#ae81ff>300</span> -segment_format mp4 -segment_atclocktime <span style=color:#ae81ff>1</span> -strftime <span style=color:#ae81ff>1</span> -x264-params keyint<span style=color:#f92672>=</span>100:scenecut<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> -g <span style=color:#ae81ff>100</span> -muxdelay <span style=color:#ae81ff>0</span> -muxpreload <span style=color:#ae81ff>0</span> -reset_timestamps <span style=color:#ae81ff>1</span> -c:v h264_nvenc /srv/target_dir/segments/Segment_%Y-%m-%dT%H:%M:%S_Driveway.mp4
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl -n cameranator exec -it segment-statefulset-1 -c segment-generator -- bash -c <span style=color:#e6db74>&#34;ps aux | grep ffmpeg | grep -v grep&#34;</span>
</span></span><span style=display:flex><span>root          <span style=color:#ae81ff>14</span>  9.6  1.3 <span style=color:#ae81ff>9451976</span> <span style=color:#ae81ff>212496</span> ?      Sl   00:34 133:13 ffmpeg -hwaccel cuda -c:v h264_cuvid -rtsp_transport tcp -i rtsp://192.168.137.32:554/Streaming/Channels/101 -c copy -map <span style=color:#ae81ff>0</span> -f segment -segment_time <span style=color:#ae81ff>300</span> -segment_format mp4 -segment_atclocktime <span style=color:#ae81ff>1</span> -strftime <span style=color:#ae81ff>1</span> -x264-params keyint<span style=color:#f92672>=</span>100:scenecut<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> -g <span style=color:#ae81ff>100</span> -muxdelay <span style=color:#ae81ff>0</span> -muxpreload <span style=color:#ae81ff>0</span> -reset_timestamps <span style=color:#ae81ff>1</span> -c:v h264_nvenc /srv/target_dir/segments/Segment_%Y-%m-%dT%H:%M:%S_FrontDoor.mp4
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl -n cameranator exec -it segment-statefulset-2 -c segment-generator -- bash -c <span style=color:#e6db74>&#34;ps aux | grep ffmpeg | grep -v grep&#34;</span>
</span></span><span style=display:flex><span>root          <span style=color:#ae81ff>13</span>  8.6  1.1 <span style=color:#ae81ff>21660656</span> <span style=color:#ae81ff>92312</span> ?      Sl   00:33 119:28 ffmpeg -hwaccel cuda -c:v h264_cuvid -rtsp_transport tcp -i rtsp://192.168.137.33:554/Streaming/Channels/101 -c copy -map <span style=color:#ae81ff>0</span> -f segment -segment_time <span style=color:#ae81ff>300</span> -segment_format mp4 -segment_atclocktime <span style=color:#ae81ff>1</span> -strftime <span style=color:#ae81ff>1</span> -x264-params keyint<span style=color:#f92672>=</span>100:scenecut<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> -g <span style=color:#ae81ff>100</span> -muxdelay <span style=color:#ae81ff>0</span> -muxpreload <span style=color:#ae81ff>0</span> -reset_timestamps <span style=color:#ae81ff>1</span> -c:v h264_nvenc /srv/target_dir/segments/Segment_%Y-%m-%dT%H:%M:%S_SideGate.mp4
</span></span></code></pre></div><p>It&rsquo;d be nice if there was some sort of simple API for storing configs that had a stack-like interface; know how many configs you&rsquo;ve got,
know how many replicas you need, each replica&rsquo;s config is the same, they hit the API and pop a config and all is well.</p><h3 id=some-common-routing-patterns-with-traefik>Some common routing patterns with Traefik<a hidden class=anchor aria-hidden=true href=#some-common-routing-patterns-with-traefik>#</a></h3><p>The default ingress controller for k3s is <a href=https://traefik.io/traefik/>Traefik</a> and as an Nginx guy, I dunno how I feel about it; here&rsquo;s
what I can tell:</p><ul><li>Documentation is vast and complex yet useless and lacks good examples</li><li>Community is so-so (hard to find good examples)</li><li>It seems to get the job done</li></ul><h4 id=match-on-a-subdomain-and-subpath>Match on a subdomain and subpath<a hidden class=anchor aria-hidden=true href=#match-on-a-subdomain-and-subpath>#</a></h4><p>In my case <code>cameranator.initialed85.cc/filebrowser</code> needs to go to <code>filebrowser/filebrowser</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>traefik.containo.us/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IngressRoute</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>cameranator</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>filebrowser-ingressroute</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>routes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>match</span>: <span style=color:#ae81ff>HostRegexp(`{subdomain:cameranator}.{any:.*}`) &amp;&amp; PathPrefix(`/filebrowser`)</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Rule</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>services</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>filebrowser</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div><h4 id=match-on-a-subdomain-and-subpath-but-strip-the-subpath-before-proxying>Match on a subdomain and subpath but strip the subpath before proxying<a hidden class=anchor aria-hidden=true href=#match-on-a-subdomain-and-subpath-but-strip-the-subpath-before-proxying>#</a></h4><p>In my case <code>cameranator.initialed85.cc/api</code> needs to go to <code>hasura</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>traefik.containo.us/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Middleware</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>cameranator</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>hasura-middleware-stripprefix</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>stripPrefix</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>prefixes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#ae81ff>/api</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>traefik.containo.us/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>IngressRoute</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>cameranator</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>hasura-ingressroute</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>routes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>match</span>: <span style=color:#ae81ff>HostRegexp(`{subdomain:cameranator}.{any:.*}`) &amp;&amp; PathPrefix(`/api`)</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Rule</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>middlewares</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>hasura-middleware-stripprefix</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>services</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>hasura</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div><h3 id=accessing-an-arduino-via-usb-serial-port>Accessing an Arduino via USB serial port<a hidden class=anchor aria-hidden=true href=#accessing-an-arduino-via-usb-serial-port>#</a></h3><p>This one is pretty easy to be honest; you just describe a volume with a <code>hostPath</code> specified and then mount it to a container as you would
anything.</p><p>Additionally, because there&rsquo;s only one node that has the Arduino plugged in, I added a label of <code>iot-role=sprinklers</code> to that node and used
at as a <code>nodeSelector</code> in my deployment.</p><p><a href=https://github.com/initialed85/mqtt_things/blob/master/kubernetes/sprinklers-cli.yaml>See here for the details</a>.</p><h3 id=exposing-a-non-kubernetes-service-as-a-kubernetes-service>Exposing a non-Kubernetes service as a Kubernetes service<a hidden class=anchor aria-hidden=true href=#exposing-a-non-kubernetes-service-as-a-kubernetes-service>#</a></h3><p>To ease my migration path, I wanted to keep using my non-Kubernetes MQTT broker until I&rsquo;m ready to cut it over; this is also very easy.</p><p>You declare a service as you would for anything but you don&rsquo;t set any selectors; then you manually declare an endpoint and point it at your
non-Kubernetes service and boom, you can hit that thing from anyhwere in your cluster using Kubernetes DNS semantics and plus when you&rsquo;re
ready to cut it over, you don&rsquo;t have to change anything that&rsquo;s pointing to it as that interface stays the same.</p><p>Here&rsquo;s the scoop:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>mqtt-things</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>mqtt-broker</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>port</span>: <span style=color:#ae81ff>1883</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>1883</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Endpoints</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>mqtt-things</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>mqtt-broker</span>
</span></span><span style=display:flex><span><span style=color:#f92672>subsets</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>addresses</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>ip</span>: <span style=color:#ae81ff>192.168.137.253</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>1883</span>
</span></span></code></pre></div><p>And <a href=https://github.com/initialed85/mqtt_things/blob/master/kubernetes/mqtt-broker.yaml>the real code is here</a>.</p><h3 id=multi-arch-docker-builds>Multi-arch Docker builds<a hidden class=anchor aria-hidden=true href=#multi-arch-docker-builds>#</a></h3><p>This one enables me to ship my MQTT stuff either to my x86 or my ARM nodes (except for the Arduino-reliant workload of course); it&rsquo;s pretty
straightforward:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>set -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>docker buildx create --use --name mqtt_things
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> cleanup<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  docker buildx rm mqtt_things <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>trap cleanup EXIT
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> build<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  _<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>1?:first argument must be CMD_NAME<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>  _<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>2?:second argument must be Docker image name part<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  docker buildx build <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --platform linux/amd64,linux/arm64 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --build-arg CMD_NAME<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>1<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -f docker/cli/Dockerfile <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -t <span style=color:#e6db74>&#34;initialed85/mqtt-things-</span><span style=color:#e6db74>${</span>2<span style=color:#e6db74>}</span><span style=color:#e6db74>:latest&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --push <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    .
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>build <span style=color:#e6db74>&#34;sensors_cli&#34;</span> <span style=color:#e6db74>&#34;sensors-cli&#34;</span>
</span></span><span style=display:flex><span>build <span style=color:#e6db74>&#34;smart_aircons_cli&#34;</span> <span style=color:#e6db74>&#34;smart-aircons-cli&#34;</span>
</span></span><span style=display:flex><span>build <span style=color:#e6db74>&#34;sprinklers_cli&#34;</span> <span style=color:#e6db74>&#34;sprinklers-cli&#34;</span>
</span></span></code></pre></div><p><a href=https://github.com/initialed85/mqtt_things/blob/master/build_tag_and_push.sh>Real code here</a>.</p><h3 id=deploying-pushing-to-and-pulling-from-a-private-docker-registry>Deploying, pushing to and pulling from a private Docker registry<a hidden class=anchor aria-hidden=true href=#deploying-pushing-to-and-pulling-from-a-private-docker-registry>#</a></h3><p>Until this point I had just been using my free tier Docker Hub repositories but eventually I needed to deploy my landing page Nginx
container which has some baked in credentials; the logical choice is to run a private registry.</p><p>First, I needed to literally ship the private registry- fortunately there&rsquo;s
a <a href=https://rook.io/docs/rook/v1.10/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/#consume-the-shared-filesystem-k8s-registry-sample>great example as part of the Rook Ceph documentation</a>
that of course uses the storage cluster; I made
a <a href=https://gist.github.com/initialed85/1db5af337fed4ad9784fee725f551fb5>slight variation that includes an external port 5000 presence</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f https://gist.githubusercontent.com/initialed85/1db5af337fed4ad9784fee725f551fb5/raw/e9ea67134b6a4bb9ae3257edb9eb1503937ff100/kube-registry.yaml
</span></span></code></pre></div><p>Then you&rsquo;ll need to modify your build machine&rsquo;s <code>/etc/hosts</code> to look a bit like this (replacing the IP with one of your node IPs of course):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>192.168.137.253  kube-registry
</span></span></code></pre></div><p>And modify your build machine&rsquo;s Docker <code>daemon.json</code> or MacOS preference menu equivalent like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;insecure-registries&#34;</span>: [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;kube-registry:5000&#34;</span>
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Ensure on each of your nodes you have a file at <code>/etc/rancher/k3s/registries.yaml</code> that reads as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>mirrors:
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;kube-registry:5000&#34;</span>:
</span></span><span style=display:flex><span>    endpoint:
</span></span><span style=display:flex><span>      - <span style=color:#e6db74>&#34;http://kube-registry:5000&#34;</span>
</span></span></code></pre></div><p>And you&rsquo;ll also need to edit <code>/ect/hosts</code> on each node to look a bit like this (replacing the IP with that node&rsquo;s IP of course):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>192.168.137.253	kube-registry
</span></span></code></pre></div><p>Finally you&rsquo;re set up to have a build script that looks like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>set -e -x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pushd <span style=color:#e6db74>&#34;</span><span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>docker build <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -t kube-registry:5000/nginx-router:latest <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -f ./Dockerfile <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  .
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>docker push kube-registry:5000/nginx-router:latest
</span></span></code></pre></div><p>And a deployment that looks like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>nginx-router</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-deployment</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>image</span>: <span style=color:#ae81ff>kube-registry:5000/nginx-router:latest</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>containerPort</span>: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><h2 id=closing-thoughts>Closing thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>Honestly, I&rsquo;m pretty pleased- functionally, my videos are better quality now (having the single HP server do all the recording and
transcoding alone seemed to result in weird video artefacts), they load faster now and I <em>think</em> the distributed storage is faster (mixture
of SSDs and HDDs) than my dying all-HDD ZFS pool was (fear not, it will live again soon as even more storage given to the storage cluster).</p><p>I&rsquo;ll migrate the other stuff over in the coming weeks, if I come across anything really interesting I&rsquo;ll write another article.</p><p>Probable road map stuff:</p><ul><li>I&rsquo;d like to try to deploy a master-master Postgres cluster<ul><li>I&rsquo;ll move the Cameranator DB over to it</li><li>I&rsquo;ll probably also move Home Assistant over to it</li></ul></li><li>I guess I should have some IaC</li></ul><p>I&rsquo;ve definitely enjoyed learning about Kubernetes, I think as complex as it is, it&rsquo;s probably the future.</p><p>Stay tuned for 3 months from now when I&rsquo;m migrating everything to <a href=https://www.nomadproject.io/>Nomad</a> ho ho ho.</p></div><footer class=post-footer><ul class=post-tags></ul></footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//initialed85.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2022 <a href=https://initialed85.cc/>initialed85's misc tech stuff</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>