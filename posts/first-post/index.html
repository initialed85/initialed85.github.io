<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Troubleshooting a mysterious Python test failure | initialed85's misc tech stuff</title><meta name=keywords content><meta name=description content="Context
At my day job my colleagues and I develop a data gathering and visualisation platform that has a fair bit of Python behind the scenes.
We test a lot of this Python using pytest and all our tests are run by a large locally hosted instance of TeamCity.
A test run involves TeamCity executing a bash script responsible for setting up any test dependencies (usually Docker containers) and then executing the test itself (also usually a Docker container)."><meta name=author content><link rel=canonical href=https://initialed85.github.io/posts/first-post/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://initialed85.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://initialed85.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://initialed85.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://initialed85.github.io/apple-touch-icon.png><link rel=mask-icon href=https://initialed85.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Troubleshooting a mysterious Python test failure"><meta property="og:description" content="Context
At my day job my colleagues and I develop a data gathering and visualisation platform that has a fair bit of Python behind the scenes.
We test a lot of this Python using pytest and all our tests are run by a large locally hosted instance of TeamCity.
A test run involves TeamCity executing a bash script responsible for setting up any test dependencies (usually Docker containers) and then executing the test itself (also usually a Docker container)."><meta property="og:type" content="article"><meta property="og:url" content="https://initialed85.github.io/posts/first-post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-09-13T20:31:01+08:00"><meta property="article:modified_time" content="2022-09-13T20:31:01+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Troubleshooting a mysterious Python test failure"><meta name=twitter:description content="Context
At my day job my colleagues and I develop a data gathering and visualisation platform that has a fair bit of Python behind the scenes.
We test a lot of this Python using pytest and all our tests are run by a large locally hosted instance of TeamCity.
A test run involves TeamCity executing a bash script responsible for setting up any test dependencies (usually Docker containers) and then executing the test itself (also usually a Docker container)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://initialed85.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Troubleshooting a mysterious Python test failure","item":"https://initialed85.github.io/posts/first-post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Troubleshooting a mysterious Python test failure","name":"Troubleshooting a mysterious Python test failure","description":"Context\nAt my day job my colleagues and I develop a data gathering and visualisation platform that has a fair bit of Python behind the scenes.\nWe test a lot of this Python using pytest and all our tests are run by a large locally hosted instance of TeamCity.\nA test run involves TeamCity executing a bash script responsible for setting up any test dependencies (usually Docker containers) and then executing the test itself (also usually a Docker container).","keywords":[],"articleBody":"Context\nAt my day job my colleagues and I develop a data gathering and visualisation platform that has a fair bit of Python behind the scenes.\nWe test a lot of this Python using pytest and all our tests are run by a large locally hosted instance of TeamCity.\nA test run involves TeamCity executing a bash script responsible for setting up any test dependencies (usually Docker containers) and then executing the test itself (also usually a Docker container).\nProblem\nMy interest was piqued by one of my colleagues trying to understand why a particular long-running end-to-end test had suddenly started failing part-way through with seemingly no relevant changes to any nearby code.\nTeamCity gave us some clear information as to when the trouble started:\nThe TeamCity build log doesn’t show a lot beyond the test execution suddenly stopping with an ominous Killed message:\nEnabling “Verbose” mode in the build log output for TeamCity gives us a little more context and clarifies that the test container died suddenly:\nFirst thoughts\nLeaning on some prior knowledge, I’m reasonably confident of the following:\nThe stdout of a process will read Killed when some other process kills a process (such as the kill command) This was observed in the build log TeamCity will show related stop and possible kill Docker events preceding a die Docker event if the container was request to stop ( such as using the docker stop command) This was not observed in the build log When things get killed at random, my mind immediately goes to the Linux OOM Killer (a function of the Linux kernel dedicated to identifying and killing memory hogging processes).\nProcesses running inside Docker containers on Linux are namespaced to achieve what feels like isolation to the process in question; to the OOM Killer though, a process is just a process and if it’s using too much memory it’s going to get killed to save the system.\nDigging deeper\nUnfortunately the TeamCity runner VMs had all been rebooted (redeployed in fact) since the last failing test run as part of some ongoing improvements to our CI system and so any incriminating dmesg logs were long gone.\nFortunately we have some long-term metrics as part of our Grafana / Prometheus / VictoriaMetrics deployment that shows some OOM Killer activity for that particular TeamCity runner VM:\nWhile not exactly a smoking gun, it definitely suggests that we run out of memory on that (and probably all) TeamCity runner VMs from time-to-time.\nHere are the specs for that VM according to our Proxmox cluster:\nYou’ll have to trust me on this next statement as I didn’t grab a screenshot: there was no swap configured- this is relevant, as depending on the amount of memory needed and the amount our test was likely to be affected by bad performance, having swap configured could potentially absorb memory leaks and permit our test to pass.\nRecreating the problem\nWith a strong hypothesis formed, it’s time to set about proving it- not wanting to spend much effort on potentially throwaway scaffolding, I decided just to run the test locally (in Docker, but for macOS) and just watch its memory usage in the meantime.\nThat looked a bit like this in htop (note the memory percentage increasing, excuse the GIF encoding):\nAfter some quick Googling on how best to do memory profiling on a Python process without having to edit the actual code I came across memray which appealed to me for two reasons:\nI could use it as a wrapper to execute my tests It has had recent commits Installing and running it was a trifle:\npip install memray python -m memray run -o output.bin \\ -m pytest -vv e2e/tests/socket_api_vs_db_stat_import_server_test.py After the test completed, memray tells us what to run to generate a flamegraph that we can then open:\npython -m memray flamegraph output.bin open memray-flamegraph-output.html The result is quite interesting:\nWe can see that the largest 3 items are all related to logging; there’s nothing particularly interesting about the way we do logging (but we do a lot of it)- the items in question are some of the larger log entries (e.g. dictionaries being repr’d).\nIf we check the summary to see the overall damage:\nOh it’s bad.\nIt’s at this point I recall that pytest keeps tracks of all Python loggers and includes their messages in the test output (something I’ve intentionally disabled before in favour of live-logging to get context on long-running tests)- could it be that the sheer volume of log messages and the extended duration of the tests is using up all the RAM?\nTesting the hypothesis\nThis is simple enough- we just have to re-run the test with the log capturing disabled (you probably don’t need all of these flags, but my initial attempts with just --capture=no didn’t work so I went scorched earth with the flags):\npython -m memray run -o output.bin \\ -m pytest -vv \\ --capture=no --show-capture=no --log-level=CRITICAL \\ --log-cli-level=CRITICAL -o junit_logging=no \\ e2e/tests/socket_api_vs_db_stat_import_server_test.py python -m memray flamegraph output.bin open memray-flamegraph-output.html The flamegraph shape more closely represents the work being done (lots of serialisation of data between Go and Python):\nAnd the summary reflects far less memory usage:\nResolution\nWith a pretty clear cut smoking gun, we can set about fixing the problem; here are a few options available:\nInside our platform Remove or edit those large log entries (if they’re not useful) Stub out the logger with a dummy logger (not actually a logger for pytest purposes but feels like one to our code) Outside our platform Change to another Python test runner Just disable log recording (as we did to test the hypothesis) For this particular situation, the logs caught by pytest are not that useful- they’re usually just logs showing repeated RPC failures or empty query / request results while waiting for them to be non-empty, so it’s acceptable to simply check-in the extra pytest flags for disabling log recording and call it a day!\n","wordCount":"995","inLanguage":"en","datePublished":"2022-09-13T20:31:01+08:00","dateModified":"2022-09-13T20:31:01+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://initialed85.github.io/posts/first-post/"},"publisher":{"@type":"Organization","name":"initialed85's misc tech stuff","logo":{"@type":"ImageObject","url":"https://initialed85.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://initialed85.github.io/ accesskey=h title="initialed85's misc tech stuff (Alt + H)">initialed85's misc tech stuff</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Troubleshooting a mysterious Python test failure</h1><div class=post-meta><span title='2022-09-13 20:31:01 +0800 +0800'>September 13, 2022</span></div></header><div class=post-content><p><strong>Context</strong></p><p>At my day job my colleagues and I develop a <a href=https://www.ftpsolutions.com.au/products/ims/>data gathering and visualisation platform</a> that
has a fair bit of <a href=https://www.python.org/>Python</a> behind the scenes.</p><p>We test a lot of this Python using <a href=https://docs.pytest.org/en/7.1.x/>pytest</a> and all our tests are run by a large locally hosted instance
of <a href=https://www.jetbrains.com/teamcity/>TeamCity</a>.</p><p>A test run involves TeamCity executing a <a href=https://www.gnu.org/software/bash/>bash</a> script responsible for setting up any test
dependencies (usually <a href=https://www.docker.com/>Docker</a> containers) and then executing the test itself (also usually a Docker container).</p><p><strong>Problem</strong></p><p>My interest was piqued by one of my colleagues trying to understand why a
particular long-running <a href=https://www.testim.io/blog/end-to-end-testing-guide/>end-to-end test</a> had suddenly started failing part-way
through with seemingly no relevant changes to any nearby code.</p><p>TeamCity gave us some clear information as to when the trouble started:</p><p><img loading=lazy src=/posts/first-post/image-1.png alt="Image 1"></p><p>The TeamCity <a href=https://www.jetbrains.com/help/teamcity/build-log.html#Viewing+Build+Log>build log</a> doesn&rsquo;t show a lot beyond the test
execution suddenly stopping with an ominous <code>Killed</code> message:</p><p><img loading=lazy src=/posts/first-post/image-2.png alt="Image 2"></p><p>Enabling &ldquo;Verbose&rdquo; mode in the build log output for TeamCity gives us a little more context and clarifies that the test container died
suddenly:</p><p><img loading=lazy src=/posts/first-post/image-3.png alt="Image 3"></p><p><strong>First thoughts</strong></p><p>Leaning on some prior knowledge, I&rsquo;m reasonably confident of the following:</p><ul><li>The <a href=https://en.wikipedia.org/wiki/Standard_streams#Standard_output_(stdout)>stdout</a> of a process will read <code>Killed</code> when some other
process kills a process (such as the <a href=https://man7.org/linux/man-pages/man2/kill.2.html>kill</a> command)<ul><li>This was observed in the build log</li></ul></li><li>TeamCity will show related <code>stop</code> and possible <code>kill</code> Docker events preceding a <code>die</code> Docker event if the container was request to stop (
such as using the <a href=https://docs.docker.com/engine/reference/commandline/stop/>docker stop</a> command)<ul><li>This <em>was not</em> observed in the build log</li></ul></li></ul><p>When things get killed at random, my mind immediately goes to
the <a href=https://en.wikipedia.org/wiki/Out_of_memory#Out_of_memory_management>Linux OOM Killer</a> (a function of the Linux kernel dedicated to
identifying and killing memory hogging processes).</p><p>Processes running inside Docker containers on Linux are <a href=https://en.wikipedia.org/wiki/Linux_namespaces>namespaced</a> to achieve what feels
like isolation to the process in question; to the OOM Killer though, a process is just a process and if it&rsquo;s using too much memory it&rsquo;s
going to get killed to save the system.</p><p><strong>Digging deeper</strong></p><p>Unfortunately the TeamCity runner VMs had all been rebooted (redeployed in fact) since the last failing test run as part of some ongoing
improvements to our <a href=https://en.wikipedia.org/wiki/Continuous_integration>CI</a> system and so any
incriminating <a href=https://man7.org/linux/man-pages/man1/dmesg.1.html>dmesg</a> logs were long gone.</p><p>Fortunately we have some long-term metrics as part of our <a href=https://grafana.com/>Grafana</a> / <a href=https://prometheus.io/>Prometheus</a>
/ <a href=https://victoriametrics.com/>VictoriaMetrics</a> deployment that shows some OOM Killer activity for that particular TeamCity runner VM:</p><p><img loading=lazy src=/posts/first-post/image-4.png alt="Image 4"></p><p>While not exactly a smoking gun, it definitely suggests that we run out of memory on that (and probably all) TeamCity runner VMs from
time-to-time.</p><p>Here are the specs for that VM according to our <a href=https://www.proxmox.com/en/>Proxmox</a> cluster:</p><p><img loading=lazy src=/posts/first-post/image-5.png alt="Image 5"></p><p>You&rsquo;ll have to trust me on this next statement as I didn&rsquo;t grab a screenshot: there was no swap configured- this is relevant, as
depending on the amount of memory needed and the amount our test was likely to be affected by bad performance, having swap configured could
potentially absorb memory leaks and permit our test to pass.</p><p><strong>Recreating the problem</strong></p><p>With a strong hypothesis formed, it&rsquo;s time to set about proving it- not wanting to spend much effort on potentially throwaway scaffolding, I
decided just to run the test locally (in Docker, but for macOS) and just watch its memory usage in the meantime.</p><p>That looked a bit like this in <a href=https://htop.dev/>htop</a> (note the memory percentage increasing, excuse the GIF encoding):</p><p><img loading=lazy src=/posts/first-post/video-1.gif alt="Video 1"></p><p>After some quick Googling on how best to do memory profiling on a Python process without having to edit the actual code I came
across <a href=https://github.com/bloomberg/memray>memray</a> which appealed to me for two reasons:</p><ul><li>I could <a href=https://github.com/bloomberg/memray#usage>use it as a wrapper</a> to execute my tests</li><li>It has had <a href=https://github.com/bloomberg/memray/commits/main>recent commits</a></li></ul><p>Installing and running it was a trifle:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pip install memray
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>python -m memray run -o output.bin <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -m pytest -vv e2e/tests/socket_api_vs_db_stat_import_server_test.py
</span></span></code></pre></div><p>After the test completed, <code>memray</code> tells us what to run to generate a <a href=https://github.com/brendangregg/FlameGraph>flamegraph</a> that we can
then open:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>python -m memray flamegraph output.bin
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>open memray-flamegraph-output.html
</span></span></code></pre></div><p>The result is quite interesting:</p><p><img loading=lazy src=/posts/first-post/image-6.png alt="Image 6"></p><p>We can see that the largest 3 items are all related to logging; there&rsquo;s nothing particularly interesting about the way we do logging (but we
do a lot of it)- the items in question are some of the larger log entries (e.g. dictionaries
being <a href=https://docs.python.org/3/library/functions.html#repr>repr&rsquo;d</a>).</p><p>If we check the summary to see the overall damage:</p><p><img loading=lazy src=/posts/first-post/image-7.png alt="Image 7"></p><p>Oh it&rsquo;s bad.</p><p>It&rsquo;s at this point I recall
that <code>pytest</code> <a href=https://docs.pytest.org/en/7.1.x/how-to/logging.html#how-to-manage-logging>keeps tracks of all Python loggers</a> and includes
their messages in the test output (something I&rsquo;ve intentionally disabled before in favour of live-logging to get context on long-running
tests)- could it be that the sheer volume of log messages and the extended duration of the tests is using up all the RAM?</p><p><strong>Testing the hypothesis</strong></p><p>This is simple enough- we just have to re-run the test with the log capturing disabled (you probably don&rsquo;t need all of these flags, but my
initial attempts with just <code>--capture=no</code> didn&rsquo;t work so I went scorched earth with the flags):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>python -m memray run -o output.bin <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -m pytest -vv <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --capture<span style=color:#f92672>=</span>no --show-capture<span style=color:#f92672>=</span>no --log-level<span style=color:#f92672>=</span>CRITICAL <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --log-cli-level<span style=color:#f92672>=</span>CRITICAL -o junit_logging<span style=color:#f92672>=</span>no <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  e2e/tests/socket_api_vs_db_stat_import_server_test.py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>python -m memray flamegraph output.bin
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>open memray-flamegraph-output.html
</span></span></code></pre></div><p>The flamegraph shape more closely represents the work being done (lots
of <a href=https://github.com/ftpsolutions/gomssql-python/>serialisation of data between Go and Python</a>):</p><p><img loading=lazy src=/posts/first-post/image-8.png alt="Image 8"></p><p>And the summary reflects far less memory usage:</p><p><img loading=lazy src=/posts/first-post/image-9.png alt="Image 9"></p><p><strong>Resolution</strong></p><p>With a pretty clear cut smoking gun, we can set about fixing the problem; here are a few options available:</p><ul><li>Inside our platform<ul><li>Remove or edit those large log entries (if they&rsquo;re not useful)</li><li>Stub out the logger with a dummy logger (not actually a logger for <code>pytest</code> purposes but feels like one to our code)</li></ul></li><li>Outside our platform<ul><li>Change to another Python test runner</li><li>Just disable log recording (as we did to test the hypothesis)</li></ul></li></ul><p>For this particular situation, the logs caught by <code>pytest</code> are not that useful- they&rsquo;re usually just logs showing repeated RPC failures or
empty query / request results while waiting for them to be non-empty, so it&rsquo;s acceptable to simply check-in the extra <code>pytest</code> flags for
disabling log recording and call it a day!</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://initialed85.github.io/>initialed85's misc tech stuff</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>